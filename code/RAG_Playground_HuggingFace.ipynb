{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876dc829",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Playground - Hugging Face Online Models\n",
    "\n",
    "> This notebook acts as a tool for users to play around with vectorizing documents and using a RAG architecture to improve the responses and capabilities of an AI (LLM) for some unique purpose. This version uses **Hugging Face online models** instead of local models, making it accessible to anyone without requiring specific local model installations.\n",
    ">\n",
    "> **Key Features:**\n",
    "> - Uses Hugging Face transformers pipeline for easy model access\n",
    "> - Works with free Hugging Face models (no account required for many models)\n",
    "> - Automatically handles model downloading and caching\n",
    "> - Supports both CPU and GPU execution\n",
    "> - Uses sentence-transformers for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad57f93",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "> First, let's make sure we have all the required packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('==')[0])\n",
    "        print(f\"‚úì {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Essential packages for this notebook\n",
    "packages = [\n",
    "    \"transformers>=4.30.0\",\n",
    "    \"torch\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",  # Use faiss-gpu if you have GPU support\n",
    "    \"langchain\",\n",
    "    \"langchain-community\",\n",
    "    \"gradio\",\n",
    "    \"pypdf\",\n",
    "    \"datasets\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd0d8d",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "> Import all necessary libraries and set up our environment for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from typing import List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers and model loading\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    TextStreamer\n",
    ")\n",
    "\n",
    "# Sentence transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LangChain for document processing and vector stores\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Other utilities\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7cbf2",
   "metadata": {},
   "source": [
    "## Configure Hugging Face Online RAG Assistant\n",
    "\n",
    "> This class mimics the functionality of the original AMAS_RAG_Assistant but uses Hugging Face online models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d127b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceRAGAssistant:\n",
    "    \"\"\"\n",
    "    A RAG Assistant that uses Hugging Face online models instead of local ones.\n",
    "    This makes it accessible to anyone without requiring specific local model installations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/DialoGPT-medium\",  # Default conversational model\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        device: str = \"auto\",  # \"auto\", \"cpu\", or \"cuda\"\n",
    "        max_new_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "        use_auth_token: Optional[str] = None,  # Optional HF token for gated models\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.device = self._setup_device(device)\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.do_sample = do_sample\n",
    "        self.use_auth_token = use_auth_token\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        self.embedding_model = None\n",
    "        self.vector_store = None\n",
    "        self.documents = []\n",
    "        \n",
    "        # RAG settings\n",
    "        self.rag_mode = False\n",
    "        self.k = 3  # Number of documents to retrieve\n",
    "        self.min_score = 0.0  # Minimum similarity score\n",
    "        \n",
    "        # Load models\n",
    "        self._load_language_model()\n",
    "        self._load_embedding_model()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"‚úÖ HuggingFaceRAGAssistant initialized successfully!\")\n",
    "            print(f\"üìù Language Model: {self.model_name}\")\n",
    "            print(f\"üîç Embedding Model: {self.embedding_model_name}\")\n",
    "            print(f\"üñ•Ô∏è  Device: {self.device}\")\n",
    "    \n",
    "    def _setup_device(self, device: str) -> str:\n",
    "        \"\"\"Setup the appropriate device for computation.\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def _load_language_model(self):\n",
    "        \"\"\"Load the language model and tokenizer from Hugging Face.\"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"üîÑ Loading language model: {self.model_name}\")\n",
    "            \n",
    "            # Create a text generation pipeline\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=0 if self.device == \"cuda\" and torch.cuda.is_available() else -1,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                use_auth_token=self.use_auth_token,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"‚úÖ Language model loaded successfully!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading language model: {e}\")\n",
    "            print(\"üîÑ Falling back to a smaller model...\")\n",
    "            # Fallback to a smaller, more reliable model\n",
    "            self.model_name = \"gpt2\"\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                device=0 if self.device == \"cuda\" and torch.cuda.is_available() else -1\n",
    "            )\n",
    "    \n",
    "    def _load_embedding_model(self):\n",
    "        \"\"\"Load the embedding model for vector search.\"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"üîÑ Loading embedding model: {self.embedding_model_name}\")\n",
    "            \n",
    "            # Use HuggingFaceEmbeddings for LangChain compatibility\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': self.device},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"‚úÖ Embedding model loaded successfully!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading embedding model: {e}\")\n",
    "            # Fallback to a smaller embedding model\n",
    "            self.embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': self.device}\n",
    "            )\n",
    "    \n",
    "    def process_pdf_to_vector_store(\n",
    "        self, \n",
    "        pdf_path: str, \n",
    "        chunk_size: int = 1000, \n",
    "        chunk_overlap: int = 200\n",
    "    ) -> Tuple[List[Document], any]:\n",
    "        \"\"\"\n",
    "        Process a PDF file and create a vector store.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (documents, vector_store)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"üîÑ Processing PDF: {pdf_path}\")\n",
    "            \n",
    "            # Load PDF\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"üìÑ Loaded {len(documents)} pages from PDF\")\n",
    "            \n",
    "            # Split documents into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "            )\n",
    "            \n",
    "            split_documents = text_splitter.split_documents(documents)\n",
    "            self.documents = split_documents\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"‚úÇÔ∏è  Split into {len(split_documents)} chunks\")\n",
    "            \n",
    "            # Create vector store\n",
    "            self.vector_store = FAISS.from_documents(\n",
    "                split_documents, \n",
    "                self.embedding_model\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"‚úÖ Vector store created successfully!\")\n",
    "            \n",
    "            return split_documents, self.vector_store\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing PDF: {e}\")\n",
    "            return [], None\n",
    "    \n",
    "    def query_vector_store(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: Optional[int] = None, \n",
    "        min_score: Optional[float] = None\n",
    "    ) -> Tuple[List[Document], List[float]]:\n",
    "        \"\"\"\n",
    "        Query the vector store for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of documents to retrieve\n",
    "            min_score: Minimum similarity score\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (documents, scores)\n",
    "        \"\"\"\n",
    "        if self.vector_store is None:\n",
    "            print(\"‚ùå No vector store available. Please process a document first.\")\n",
    "            return [], []\n",
    "        \n",
    "        k = k or self.k\n",
    "        min_score = min_score or self.min_score\n",
    "        \n",
    "        try:\n",
    "            # Search for similar documents\n",
    "            docs_and_scores = self.vector_store.similarity_search_with_score(\n",
    "                query, k=k\n",
    "            )\n",
    "            \n",
    "            # Filter by minimum score if specified\n",
    "            if min_score > 0:\n",
    "                docs_and_scores = [\n",
    "                    (doc, score) for doc, score in docs_and_scores \n",
    "                    if score >= min_score\n",
    "                ]\n",
    "            \n",
    "            docs = [doc for doc, score in docs_and_scores]\n",
    "            scores = [score for doc, score in docs_and_scores]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"üîç Found {len(docs)} relevant documents\")\n",
    "                for i, score in enumerate(scores):\n",
    "                    print(f\"   Document {i+1}: Similarity score = {score:.4f}\")\n",
    "            \n",
    "            return docs, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error querying vector store: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def generate_response(self, user_input: str, use_rag: Optional[bool] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to user input, optionally using RAG.\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's question or input\n",
    "            use_rag: Whether to use RAG (if None, uses self.rag_mode)\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        use_rag = use_rag if use_rag is not None else self.rag_mode\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        if use_rag and self.vector_store is not None:\n",
    "            # RAG mode: retrieve relevant documents\n",
    "            docs, scores = self.query_vector_store(user_input)\n",
    "            \n",
    "            if docs:\n",
    "                # Create context from retrieved documents\n",
    "                context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                prompt = f\"\"\"Context from documents:\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "\n",
    "Please answer the question based on the provided context. If the context doesn't contain relevant information, you may use your general knowledge but please indicate when you're doing so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            else:\n",
    "                prompt = user_input\n",
    "        else:\n",
    "            # Non-RAG mode: use input directly\n",
    "            prompt = user_input\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                top_p=self.top_p,\n",
    "                do_sample=self.do_sample,\n",
    "                pad_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            # Extract the generated text\n",
    "            generated_text = response[0]['generated_text']\n",
    "            \n",
    "            return generated_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating response: {e}\")\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "    \n",
    "    def toggle_rag_mode(self):\n",
    "        \"\"\"Toggle RAG mode on/off.\"\"\"\n",
    "        self.rag_mode = not self.rag_mode\n",
    "        mode = \"enabled\" if self.rag_mode else \"disabled\"\n",
    "        print(f\"üîÑ RAG mode {mode}\")\n",
    "        return self.rag_mode\n",
    "\n",
    "# Initialize the assistant with a lightweight model suitable for most users\n",
    "print(\"üöÄ Initializing Hugging Face RAG Assistant...\")\n",
    "print(\"üìù Using a lightweight model suitable for online use...\")\n",
    "\n",
    "# You can change these models based on your needs and computational resources\n",
    "rag_assistant = HuggingFaceRAGAssistant(\n",
    "    model_name=\"microsoft/DialoGPT-medium\",  # Good balance of quality and speed\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Fast and efficient\n",
    "    device=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f3028",
   "metadata": {},
   "source": [
    "## Alternative Model Options\n",
    "\n",
    "> Here are some alternative models you can try. Simply change the model names in the cell above:\n",
    "\n",
    "### Language Models (Text Generation):\n",
    "- **Lightweight & Fast**: `\"gpt2\"`, `\"microsoft/DialoGPT-medium\"`\n",
    "- **Better Quality**: `\"microsoft/DialoGPT-large\"`, `\"facebook/blenderbot-400M-distill\"`\n",
    "- **Advanced (requires more resources)**: `\"microsoft/DialoGPT-large\"`, `\"facebook/blenderbot-1B-distill\"`\n",
    "\n",
    "### Embedding Models:\n",
    "- **Fast**: `\"sentence-transformers/all-MiniLM-L6-v2\"`\n",
    "- **Better Quality**: `\"sentence-transformers/all-mpnet-base-v2\"`\n",
    "- **Specialized**: `\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"` (for Q&A)\n",
    "\n",
    "### Using Hugging Face Account (Optional):\n",
    "If you have a Hugging Face account and token, you can access more models:\n",
    "```python\n",
    "# Get your token from https://huggingface.co/settings/tokens\n",
    "rag_assistant = HuggingFaceRAGAssistant(\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",  # Requires HF token\n",
    "    use_auth_token=\"your_hf_token_here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c90e95",
   "metadata": {},
   "source": [
    "## Load and Process Documents\n",
    "\n",
    "> Now let's load a PDF document and create a vector store for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure document processing\n",
    "pdf_file_path = \"../data/CUI_SPEC.pdf\"  # Adjust path as needed\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(pdf_file_path):\n",
    "    print(f\"üìÅ Found PDF file: {pdf_file_path}\")\n",
    "    \n",
    "    # Process the PDF and create vector store\n",
    "    documents, vector_store = rag_assistant.process_pdf_to_vector_store(\n",
    "        pdf_path=pdf_file_path,\n",
    "        chunk_size=1000,  # Adjust based on your needs\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Processing Results:\")\n",
    "    print(f\"   üìÑ Total documents/chunks: {len(documents)}\")\n",
    "    print(f\"   üîç Vector store created: {'‚úÖ Yes' if vector_store else '‚ùå No'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå PDF file not found: {pdf_file_path}\")\n",
    "    print(\"üìù Please ensure the file exists or update the path.\")\n",
    "    print(\"üîÑ You can also upload your own PDF file to the data folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b335c5c",
   "metadata": {},
   "source": [
    "## Examine the Processed Documents\n",
    "\n",
    "> Let's take a look at what documents were created from the PDF processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b844f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if documents:\n",
    "    print(f\"üìö Examining processed documents:\")\n",
    "    print(f\"   Total chunks: {len(documents)}\")\n",
    "    \n",
    "    # Show first few document chunks\n",
    "    num_to_show = min(3, len(documents))\n",
    "    \n",
    "    for i, doc in enumerate(documents[:num_to_show]):\n",
    "        print(f\"\\nüìÑ Document Chunk {i + 1}:\")\n",
    "        print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "        print(f\"   Preview: {doc.page_content[:200]}...\")\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "            print(f\"   Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå No documents to examine. Please process a PDF first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733a21f",
   "metadata": {},
   "source": [
    "## Test Vector Store Queries\n",
    "\n",
    "> Let's test how well our vector store can find relevant documents for specific queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_assistant.vector_store is not None:\n",
    "    # Test queries related to the CUI_SPEC.pdf document\n",
    "    test_queries = [\n",
    "        \"What is CUI Specified?\",\n",
    "        \"Tell me about sending CUI via email to accounts outside of Federal IT\",\n",
    "        \"What are the handling requirements for CUI?\",\n",
    "        \"What is controlled unclassified information?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üîç Testing vector store queries:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Query {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        docs, scores = rag_assistant.query_vector_store(\n",
    "            query=query,\n",
    "            k=2,  # Get top 2 most relevant documents\n",
    "            min_score=0.0\n",
    "        )\n",
    "        \n",
    "        if docs:\n",
    "            for j, (doc, score) in enumerate(zip(docs, scores)):\n",
    "                print(f\"üìÑ Result {j + 1} (Score: {score:.4f}):\")\n",
    "                # Show first 300 characters of the document\n",
    "                preview = doc.page_content[:300].replace('\\n', ' ')\n",
    "                print(f\"   {preview}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"   ‚ùå No relevant documents found\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå Vector store not available. Please process a document first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fd428",
   "metadata": {},
   "source": [
    "## Create Interactive Gradio Interface\n",
    "\n",
    "> Now let's create a user-friendly Gradio interface to interact with our RAG assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_interface():\n",
    "    \"\"\"Create a Gradio interface for the RAG assistant.\"\"\"\n",
    "    \n",
    "    def chat_with_assistant(message, use_rag, temperature, top_k, top_p, max_tokens, k_docs):\n",
    "        \"\"\"Handle chat interactions.\"\"\"\n",
    "        # Update assistant parameters\n",
    "        rag_assistant.temperature = temperature\n",
    "        rag_assistant.top_k = int(top_k)\n",
    "        rag_assistant.top_p = top_p\n",
    "        rag_assistant.max_new_tokens = int(max_tokens)\n",
    "        rag_assistant.k = int(k_docs)\n",
    "        \n",
    "        # Generate response\n",
    "        response = rag_assistant.generate_response(message, use_rag=use_rag)\n",
    "        \n",
    "        # Add mode indicator\n",
    "        mode = \"üîç RAG Mode\" if use_rag else \"ü§ñ Standard Mode\"\n",
    "        return f\"{mode}\\n\\n{response}\"\n",
    "    \n",
    "    def process_new_pdf(pdf_file, chunk_size, chunk_overlap):\n",
    "        \"\"\"Process a new PDF file.\"\"\"\n",
    "        if pdf_file is None:\n",
    "            return \"‚ùå Please upload a PDF file.\"\n",
    "        \n",
    "        try:\n",
    "            # Save uploaded file temporarily\n",
    "            temp_path = f\"temp_{pdf_file.name}\"\n",
    "            with open(temp_path, \"wb\") as f:\n",
    "                f.write(pdf_file.read())\n",
    "            \n",
    "            # Process the PDF\n",
    "            documents, vector_store = rag_assistant.process_pdf_to_vector_store(\n",
    "                pdf_path=temp_path,\n",
    "                chunk_size=int(chunk_size),\n",
    "                chunk_overlap=int(chunk_overlap)\n",
    "            )\n",
    "            \n",
    "            # Clean up temp file\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "            if documents:\n",
    "                return f\"‚úÖ Successfully processed PDF!\\nüìÑ Created {len(documents)} document chunks.\\nüîç Vector store ready for RAG queries.\"\n",
    "            else:\n",
    "                return \"‚ùå Failed to process PDF. Please try again.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error processing PDF: {str(e)}\"\n",
    "    \n",
    "    # Create the Gradio interface\n",
    "    with gr.Blocks(title=\"RAG Assistant - Hugging Face Models\", theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\"# ü§ñ RAG Assistant with Hugging Face Models\")\n",
    "        gr.Markdown(\"This interface allows you to chat with an AI assistant that can use Retrieval Augmented Generation (RAG) to answer questions based on your documents.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            # Left column - Controls\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"## ‚öôÔ∏è Settings\")\n",
    "                \n",
    "                # RAG toggle\n",
    "                rag_enabled = gr.Checkbox(\n",
    "                    label=\"üîç Enable RAG Mode\",\n",
    "                    value=False,\n",
    "                    info=\"Use document knowledge for responses\"\n",
    "                )\n",
    "                \n",
    "                # Generation parameters\n",
    "                gr.Markdown(\"### üéõÔ∏è Generation Parameters\")\n",
    "                temperature = gr.Slider(0.1, 2.0, value=0.7, label=\"Temperature\", info=\"Creativity level\")\n",
    "                top_k = gr.Slider(1, 100, value=50, label=\"Top K\", info=\"Token selection diversity\")\n",
    "                top_p = gr.Slider(0.1, 1.0, value=0.9, label=\"Top P\", info=\"Nucleus sampling\")\n",
    "                max_tokens = gr.Slider(50, 1000, value=512, label=\"Max Tokens\", info=\"Response length\")\n",
    "                \n",
    "                # RAG parameters\n",
    "                gr.Markdown(\"### üîç RAG Parameters\")\n",
    "                k_docs = gr.Slider(1, 10, value=3, label=\"K Documents\", info=\"Number of docs to retrieve\")\n",
    "                \n",
    "                # Document upload\n",
    "                gr.Markdown(\"### üìÑ Document Management\")\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                chunk_size = gr.Number(value=1000, label=\"Chunk Size\")\n",
    "                chunk_overlap = gr.Number(value=200, label=\"Chunk Overlap\")\n",
    "                process_btn = gr.Button(\"üì§ Process PDF\")\n",
    "                process_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
    "            \n",
    "            # Right column - Chat\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"## üí¨ Chat Interface\")\n",
    "                \n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(height=400, label=\"Conversation\")\n",
    "                user_input = gr.Textbox(\n",
    "                    label=\"Your Message\",\n",
    "                    placeholder=\"Ask a question or chat with the assistant...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"üì§ Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
    "                \n",
    "                # Sample questions\n",
    "                gr.Markdown(\"### üí° Sample Questions (for CUI_SPEC.pdf)\")\n",
    "                sample_questions = [\n",
    "                    \"What is CUI?\",\n",
    "                    \"How should CUI be handled when sending emails?\",\n",
    "                    \"What are the marking requirements for CUI?\",\n",
    "                    \"Explain the safeguarding requirements for CUI.\"\n",
    "                ]\n",
    "                \n",
    "                for question in sample_questions:\n",
    "                    gr.Button(question, size=\"sm\").click(\n",
    "                        lambda q=question: q, outputs=user_input\n",
    "                    )\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, history, use_rag, temp, top_k_val, top_p_val, max_tok, k_val):\n",
    "            if not message:\n",
    "                return history, \"\"\n",
    "            \n",
    "            # Get response from assistant\n",
    "            response = chat_with_assistant(message, use_rag, temp, top_k_val, top_p_val, max_tok, k_val)\n",
    "            \n",
    "            # Update chat history\n",
    "            history.append((message, response))\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Connect events\n",
    "        send_btn.click(\n",
    "            respond,\n",
    "            inputs=[user_input, chatbot, rag_enabled, temperature, top_k, top_p, max_tokens, k_docs],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            respond,\n",
    "            inputs=[user_input, chatbot, rag_enabled, temperature, top_k, top_p, max_tokens, k_docs],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        \n",
    "        process_btn.click(\n",
    "            process_new_pdf,\n",
    "            inputs=[pdf_upload, chunk_size, chunk_overlap],\n",
    "            outputs=process_status\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and display the interface\n",
    "print(\"üé® Creating Gradio interface...\")\n",
    "rag_interface = create_rag_interface()\n",
    "print(\"‚úÖ Interface created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6f98a",
   "metadata": {},
   "source": [
    "## Launch the Interactive Application\n",
    "\n",
    "> **Note**: The app will launch on a unique port. You can use it to:\n",
    "> 1. Toggle between RAG mode and standard mode\n",
    "> 2. Adjust generation parameters (temperature, top-k, top-p)\n",
    "> 3. Upload and process new PDF documents\n",
    "> 4. Ask questions and compare responses with and without RAG\n",
    "\n",
    "### How to use the interface:\n",
    "\n",
    "1. **Standard Mode**: Ask general questions using the model's built-in knowledge\n",
    "2. **RAG Mode**: Enable RAG to use document knowledge for responses\n",
    "3. **Upload PDFs**: Use the upload section to process your own documents\n",
    "4. **Adjust Parameters**: Fine-tune the model's behavior using the sliders\n",
    "\n",
    "### Tips for best results:\n",
    "- Start with RAG disabled to see baseline responses\n",
    "- Enable RAG and ask the same questions to see the difference\n",
    "- Try the sample questions provided for the CUI document\n",
    "- Experiment with different parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "if rag_interface is not None:\n",
    "    print(\"üöÄ Launching RAG Assistant interface...\")\n",
    "    print(\"üì± The interface will open in a new tab/window\")\n",
    "    print(\"üîó You can also access it through the provided local URL\")\n",
    "    \n",
    "    # Launch with sharing enabled for broader access\n",
    "    rag_interface.launch(\n",
    "        share=True,  # Creates a public link for 72 hours\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        debug=False,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå Failed to create interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to stop the interface, run this cell\n",
    "if 'rag_interface' in locals():\n",
    "    rag_interface.close()\n",
    "    print(\"üõë Interface stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bbfe4d",
   "metadata": {},
   "source": [
    "## Test RAG vs Non-RAG Responses\n",
    "\n",
    "> Let's directly compare responses with and without RAG to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_assistant.vector_store is not None:\n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"What is CUI?\",\n",
    "        \"How should I handle CUI when sending emails outside the organization?\",\n",
    "        \"What are the marking requirements for controlled unclassified information?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üî¨ Comparing RAG vs Non-RAG responses:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n‚ùì Question {i}: {question}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get response without RAG\n",
    "        print(\"ü§ñ Standard Response (No RAG):\")\n",
    "        standard_response = rag_assistant.generate_response(question, use_rag=False)\n",
    "        print(f\"   {standard_response}\")\n",
    "        \n",
    "        print(\"\\nüîç RAG-Enhanced Response:\")\n",
    "        rag_response = rag_assistant.generate_response(question, use_rag=True)\n",
    "        print(f\"   {rag_response}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå Vector store not available. Please process a document first to compare RAG vs non-RAG responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43cc91",
   "metadata": {},
   "source": [
    "## System Information and Troubleshooting\n",
    "\n",
    "> Check system resources and get troubleshooting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get system information for troubleshooting.\"\"\"\n",
    "    print(\"üñ•Ô∏è  System Information:\")\n",
    "    print(f\"   Platform: {platform.platform()}\")\n",
    "    print(f\"   Python version: {platform.python_version()}\")\n",
    "    print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
    "    print(f\"   RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    print(f\"   Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"   GPU: Not available (using CPU)\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Key Package Versions:\")\n",
    "    print(f\"   torch: {torch.__version__}\")\n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"   transformers: {transformers.__version__}\")\n",
    "    except:\n",
    "        print(\"   transformers: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import sentence_transformers\n",
    "        print(f\"   sentence-transformers: {sentence_transformers.__version__}\")\n",
    "    except:\n",
    "        print(\"   sentence-transformers: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import langchain\n",
    "        print(f\"   langchain: {langchain.__version__}\")\n",
    "    except:\n",
    "        print(\"   langchain: Not installed\")\n",
    "\n",
    "# Display system information\n",
    "get_system_info()\n",
    "\n",
    "# Show current model status\n",
    "print(f\"\\nü§ñ Current Model Status:\")\n",
    "print(f\"   Language Model: {rag_assistant.model_name}\")\n",
    "print(f\"   Embedding Model: {rag_assistant.embedding_model_name}\")\n",
    "print(f\"   Device: {rag_assistant.device}\")\n",
    "print(f\"   Vector Store: {'‚úÖ Loaded' if rag_assistant.vector_store else '‚ùå Not loaded'}\")\n",
    "print(f\"   Documents: {len(rag_assistant.documents)} chunks\")\n",
    "print(f\"   RAG Mode: {'‚úÖ Enabled' if rag_assistant.rag_mode else '‚ùå Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e48a",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "1. **Out of Memory Errors**:\n",
    "   - Use smaller models (e.g., \"gpt2\" instead of larger models)\n",
    "   - Reduce `max_new_tokens` parameter\n",
    "   - Use CPU instead of GPU if GPU memory is limited\n",
    "\n",
    "2. **Slow Response Times**:\n",
    "   - Use smaller, faster models\n",
    "   - Reduce chunk sizes when processing documents\n",
    "   - Lower the number of retrieved documents (k parameter)\n",
    "\n",
    "3. **Model Not Found Errors**:\n",
    "   - Check your internet connection\n",
    "   - Verify model names are correct\n",
    "   - Some models may require Hugging Face authentication\n",
    "\n",
    "4. **Poor RAG Performance**:\n",
    "   - Try different embedding models\n",
    "   - Adjust chunk sizes (smaller for specific questions, larger for context)\n",
    "   - Experiment with different similarity thresholds\n",
    "\n",
    "5. **Authentication Issues**:\n",
    "   - Get a free Hugging Face account and token\n",
    "   - Set the `use_auth_token` parameter when initializing the assistant\n",
    "\n",
    "### Performance Tips:\n",
    "- Start with lightweight models and upgrade as needed\n",
    "- Use GPU acceleration when available\n",
    "- Process documents in smaller batches for large files\n",
    "- Cache models locally for faster subsequent loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources (optional)\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up memory and resources.\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # Clear CUDA cache if using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"üßπ Resources cleaned up\")\n",
    "\n",
    "# Uncomment the line below if you want to clean up resources\n",
    "# cleanup_resources()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
