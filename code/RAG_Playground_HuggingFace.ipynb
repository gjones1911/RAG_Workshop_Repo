{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876dc829",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Playground - Hugging Face Online Models\n",
    "\n",
    "> This notebook acts as a tool for users to play around with vectorizing documents and using a RAG architecture to improve the responses and capabilities of an AI (LLM) for some unique purpose. This version uses **Hugging Face online models** instead of local models, making it accessible to anyone without requiring specific local model installations.\n",
    ">\n",
    "> **Key Features:**\n",
    "> - Uses Hugging Face transformers pipeline for easy model access\n",
    "> - Works with free Hugging Face models (no account required for many models)\n",
    "> - Automatically handles model downloading and caching\n",
    "> - Supports both CPU and GPU execution\n",
    "> - Uses sentence-transformers for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad57f93",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "> First, let's make sure we have all the required packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ffc024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing transformers>=4.30.0...\n",
      "Requirement already satisfied: transformers>=4.30.0 in ./ragworkshopvenv/lib/python3.10/site-packages (4.51.0)\n",
      "Requirement already satisfied: filelock in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers>=4.30.0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.30.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.30.0) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers>=4.30.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers>=4.30.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers>=4.30.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers>=4.30.0) (2025.6.15)\n",
      "✓ torch already installed\n",
      "Installing sentence-transformers...\n",
      "Requirement already satisfied: sentence-transformers in ./ragworkshopvenv/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (4.51.0)\n",
      "Requirement already satisfied: tqdm in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (0.30.1)\n",
      "Requirement already satisfied: Pillow in ./ragworkshopvenv/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: sympy in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./ragworkshopvenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.34.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing faiss-cpu...\n",
      "Requirement already satisfied: faiss-cpu in ./ragworkshopvenv/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./ragworkshopvenv/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "✓ langchain already installed\n",
      "Installing langchain-community...\n",
      "Requirement already satisfied: langchain-community in ./ragworkshopvenv/lib/python3.10/site-packages (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (0.2.12)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (0.2.34)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./ragworkshopvenv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./ragworkshopvenv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./ragworkshopvenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: anyio in ./ragworkshopvenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./ragworkshopvenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./ragworkshopvenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in ./ragworkshopvenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./ragworkshopvenv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./ragworkshopvenv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (2.18.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./ragworkshopvenv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./ragworkshopvenv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./ragworkshopvenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./ragworkshopvenv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gerald/GITS_REPOS/GIT_PLAY_GROUNDs/RAG_Workshop_Repo/code/ragworkshopvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ gradio already installed\n",
      "✓ pypdf already installed\n",
      "✓ datasets already installed\n",
      "✓ pandas already installed\n",
      "\n",
      "✅ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('==')[0])\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Essential packages for this notebook\n",
    "packages = [\n",
    "    \"transformers>=4.30.0\",\n",
    "    \"torch\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",  # Use faiss-gpu if you have GPU support\n",
    "    \"langchain\",\n",
    "    \"langchain-community\",\n",
    "    \"gradio\",\n",
    "    \"pypdf\",\n",
    "    \"datasets\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd0d8d",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "> Import all necessary libraries and set up our environment for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aecdef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 18:41:59.678190: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-07 18:41:59.714998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-07 18:42:00.372775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "🔥 PyTorch version: 2.2.1+cu121\n",
      "🚀 CUDA available: True\n",
      "🎮 GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from typing import List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers and model loading\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    TextStreamer\n",
    ")\n",
    "\n",
    "# Sentence transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# LangChain for document processing and vector stores\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Other utilities\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🚀 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20a93c-52fe-490c-aaef-fb3841996251",
   "metadata": {},
   "source": [
    "## Models with no token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965c8729-0d4f-4a51-8756-eab16aaa59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.2\n",
      "NousResearch/Nous-Hermes-2-Mistral-7B-DPO\n",
      "teknium/OpenHermes-2.5-Mistral-7B\n",
      "microsoft/phi-2\n",
      "microsoft/phi-3-mini-4k-instruct\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "OpenAccessAI/MythoMax-L2-13b\n",
      "openchat/openchat-3.5-0106\n"
     ]
    }
   ],
   "source": [
    "open_llms_no_token_required = [\n",
    "    {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"description\": \"Instruction-tuned Mistral 7B model with strong reasoning and generation capabilities.\",\n",
    "        \"pros\": [\"Excellent general-purpose LLM\", \"Performs well in RAG\", \"Fast inference\"],\n",
    "        \"cons\": [\"May require GPU with ~12GB+ VRAM for best performance\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\",\n",
    "        \"description\": \"DPO fine-tuned Mistral variant for assistant-like dialog and reasoning.\",\n",
    "        \"pros\": [\"Great for conversation and QA\", \"Performs well in long-context tasks\", \"No token needed\"],\n",
    "        \"cons\": [\"Heavier than Phi models\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
    "        \"description\": \"Mistral-based chat model tuned for helpful assistant-style responses.\",\n",
    "        \"pros\": [\"Efficient\", \"Instruction following\", \"Good with injected context (RAG)\"],\n",
    "        \"cons\": [\"May hallucinate if not grounded\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"microsoft/phi-2\",\n",
    "        \"description\": \"Compact transformer trained with curriculum learning, ideal for reasoning and basic chat.\",\n",
    "        \"pros\": [\"Lightweight\", \"Good accuracy per parameter\", \"No token required\"],\n",
    "        \"cons\": [\"Limited context window (2k)\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"microsoft/phi-3-mini-4k-instruct\",\n",
    "        \"description\": \"Latest 4k context Phi-3 model, small but powerful for structured assistant tasks.\",\n",
    "        \"pros\": [\"Efficient and very fast\", \"Strong coding and reasoning\", \"Great for edge devices\"],\n",
    "        \"cons\": [\"Limited generative depth compared to larger models\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"description\": \"Very small LLaMA-style model trained for chat and instruction following.\",\n",
    "        \"pros\": [\"Extremely lightweight\", \"Can run on CPU\", \"Good for constrained environments\"],\n",
    "        \"cons\": [\"Not as strong in generalization or long reasoning\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"OpenAccessAI/MythoMax-L2-13b\",\n",
    "        \"description\": \"A powerful LLaMA-2-based model fine-tuned for rich, open-ended conversation.\",\n",
    "        \"pros\": [\"Powerful and expressive\", \"Fine-tuned on a diverse instruction dataset\", \"No token required\"],\n",
    "        \"cons\": [\"Large (13B) - needs ~24GB+ VRAM or GGUF format\"]\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"openchat/openchat-3.5-0106\",\n",
    "        \"description\": \"Chat-focused LLaMA-based model for helpful assistant behaviors.\",\n",
    "        \"pros\": [\"Chat-optimized\", \"Can be used in RAG\", \"Open access\"],\n",
    "        \"cons\": [\"Some versions require more VRAM\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "model_names = [d[\"model\"] for d in open_llms_no_token_required]\n",
    "\n",
    "for m in model_names:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f7cbf2",
   "metadata": {},
   "source": [
    "## Configure Hugging Face Online RAG Assistant\n",
    "\n",
    "> This class mimics the functionality of the original AMAS_RAG_Assistant but uses Hugging Face online models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d127b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Hugging Face RAG Assistant...\n",
      "📝 Using a lightweight model suitable for online use...\n",
      "🔄 Loading language model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.55it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Language model loaded successfully!\n",
      "🔄 Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Embedding model loaded successfully!\n",
      "✅ HuggingFaceRAGAssistant initialized successfully!\n",
      "📝 Language Model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "🔍 Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "🖥️  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class HuggingFaceRAGAssistant:\n",
    "    \"\"\"\n",
    "    A RAG Assistant that uses Hugging Face online models instead of local ones.\n",
    "    This makes it accessible to anyone without requiring specific local model installations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/DialoGPT-medium\",  # Default conversational model\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        device: str = \"auto\",  # \"auto\", \"cpu\", or \"cuda\"\n",
    "        max_new_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "        do_sample: bool = True,\n",
    "        use_auth_token: Optional[str] = None,  # Optional HF token for gated models\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.device = self._setup_device(device)\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.do_sample = do_sample\n",
    "        self.use_auth_token = use_auth_token\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize components\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        self.embedding_model = None\n",
    "        self.vector_store = None\n",
    "        self.documents = []\n",
    "        \n",
    "        # RAG settings\n",
    "        self.rag_mode = False\n",
    "        self.k = 3  # Number of documents to retrieve\n",
    "        self.min_score = 0.0  # Minimum similarity score\n",
    "        \n",
    "        # Load models\n",
    "        self._load_language_model()\n",
    "        self._load_embedding_model()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"✅ HuggingFaceRAGAssistant initialized successfully!\")\n",
    "            print(f\"📝 Language Model: {self.model_name}\")\n",
    "            print(f\"🔍 Embedding Model: {self.embedding_model_name}\")\n",
    "            print(f\"🖥️  Device: {self.device}\")\n",
    "    \n",
    "    def _setup_device(self, device: str) -> str:\n",
    "        \"\"\"Setup the appropriate device for computation.\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def _load_language_model(self):\n",
    "        \"\"\"Load the language model and tokenizer from Hugging Face.\"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"🔄 Loading language model: {self.model_name}\")\n",
    "            \n",
    "            # Create a text generation pipeline\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=0 if self.device == \"cuda\" and torch.cuda.is_available() else -1,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                use_auth_token=self.use_auth_token,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✅ Language model loaded successfully!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading language model: {e}\")\n",
    "            print(\"🔄 Falling back to a smaller model...\")\n",
    "            # Fallback to a smaller, more reliable model\n",
    "            self.model_name = \"gpt2\"\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                device=0 if self.device == \"cuda\" and torch.cuda.is_available() else -1\n",
    "            )\n",
    "    \n",
    "    def _load_embedding_model(self):\n",
    "        \"\"\"Load the embedding model for vector search.\"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"🔄 Loading embedding model: {self.embedding_model_name}\")\n",
    "            \n",
    "            # Use HuggingFaceEmbeddings for LangChain compatibility\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': self.device},\n",
    "                encode_kwargs={'normalize_embeddings': True}\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✅ Embedding model loaded successfully!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading embedding model: {e}\")\n",
    "            # Fallback to a smaller embedding model\n",
    "            self.embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=self.embedding_model_name,\n",
    "                model_kwargs={'device': self.device}\n",
    "            )\n",
    "    \n",
    "    def process_pdf_to_vector_store(\n",
    "        self, \n",
    "        pdf_path: str, \n",
    "        chunk_size: int = 1000, \n",
    "        chunk_overlap: int = 200\n",
    "    ) -> Tuple[List[Document], any]:\n",
    "        \"\"\"\n",
    "        Process a PDF file and create a vector store.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (documents, vector_store)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"🔄 Processing PDF: {pdf_path}\")\n",
    "            \n",
    "            # Load PDF\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"📄 Loaded {len(documents)} pages from PDF\")\n",
    "            \n",
    "            # Split documents into chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "            )\n",
    "            \n",
    "            split_documents = text_splitter.split_documents(documents)\n",
    "            self.documents = split_documents\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✂️  Split into {len(split_documents)} chunks\")\n",
    "            \n",
    "            # Create vector store\n",
    "            self.vector_store = FAISS.from_documents(\n",
    "                split_documents, \n",
    "                self.embedding_model\n",
    "            )\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✅ Vector store created successfully!\")\n",
    "            \n",
    "            return split_documents, self.vector_store\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing PDF: {e}\")\n",
    "            return [], None\n",
    "    \n",
    "    def query_vector_store(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: Optional[int] = None, \n",
    "        min_score: Optional[float] = None\n",
    "    ) -> Tuple[List[Document], List[float]]:\n",
    "        \"\"\"\n",
    "        Query the vector store for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of documents to retrieve\n",
    "            min_score: Minimum similarity score\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (documents, scores)\n",
    "        \"\"\"\n",
    "        if self.vector_store is None:\n",
    "            print(\"❌ No vector store available. Please process a document first.\")\n",
    "            return [], []\n",
    "        \n",
    "        k = k or self.k\n",
    "        min_score = min_score or self.min_score\n",
    "        \n",
    "        try:\n",
    "            # Search for similar documents\n",
    "            docs_and_scores = self.vector_store.similarity_search_with_score(\n",
    "                query, k=k\n",
    "            )\n",
    "            \n",
    "            # Filter by minimum score if specified\n",
    "            if min_score > 0:\n",
    "                docs_and_scores = [\n",
    "                    (doc, score) for doc, score in docs_and_scores \n",
    "                    if score >= min_score\n",
    "                ]\n",
    "            \n",
    "            docs = [doc for doc, score in docs_and_scores]\n",
    "            scores = [score for doc, score in docs_and_scores]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"🔍 Found {len(docs)} relevant documents\")\n",
    "                for i, score in enumerate(scores):\n",
    "                    print(f\"   Document {i+1}: Similarity score = {score:.4f}\")\n",
    "            \n",
    "            return docs, scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error querying vector store: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def generate_response(self, user_input: str, use_rag: Optional[bool] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response to user input, optionally using RAG.\n",
    "        \n",
    "        Args:\n",
    "            user_input: User's question or input\n",
    "            use_rag: Whether to use RAG (if None, uses self.rag_mode)\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        use_rag = use_rag if use_rag is not None else self.rag_mode\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        if use_rag and self.vector_store is not None:\n",
    "            # RAG mode: retrieve relevant documents\n",
    "            docs, scores = self.query_vector_store(user_input)\n",
    "            \n",
    "            if docs:\n",
    "                # Create context from retrieved documents\n",
    "                context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                prompt = f\"\"\"Context from documents:\n",
    "{context}\n",
    "\n",
    "Question: {user_input}\n",
    "\n",
    "Please answer the question based on the provided context. If the context doesn't contain relevant information, you may use your general knowledge but please indicate when you're doing so.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            else:\n",
    "                prompt = user_input\n",
    "        else:\n",
    "            # Non-RAG mode: use input directly\n",
    "            prompt = user_input\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                top_p=self.top_p,\n",
    "                do_sample=self.do_sample,\n",
    "                pad_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            # Extract the generated text\n",
    "            generated_text = response[0]['generated_text']\n",
    "            \n",
    "            return generated_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating response: {e}\")\n",
    "            return \"Sorry, I encountered an error while generating a response.\"\n",
    "    \n",
    "    def toggle_rag_mode(self):\n",
    "        \"\"\"Toggle RAG mode on/off.\"\"\"\n",
    "        self.rag_mode = not self.rag_mode\n",
    "        mode = \"enabled\" if self.rag_mode else \"disabled\"\n",
    "        print(f\"🔄 RAG mode {mode}\")\n",
    "        return self.rag_mode\n",
    "\n",
    "# Initialize the assistant with a lightweight model suitable for most users\n",
    "print(\"🚀 Initializing Hugging Face RAG Assistant...\")\n",
    "print(\"📝 Using a lightweight model suitable for online use...\")\n",
    "\n",
    "\n",
    "# You can change these models based on your needs and computational resources\n",
    "rag_assistant = HuggingFaceRAGAssistant(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Good balance of quality and speed\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Fast and efficient\n",
    "    device=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f3028",
   "metadata": {},
   "source": [
    "## Alternative Model Options\n",
    "\n",
    "> Here are some alternative models you can try. Simply change the model names in the cell above:\n",
    "\n",
    "### Language Models (Text Generation):\n",
    "- **Lightweight & Fast**: `\"gpt2\"`, `\"microsoft/DialoGPT-medium\"`\n",
    "- **Better Quality**: `\"microsoft/DialoGPT-large\"`, `\"facebook/blenderbot-400M-distill\"`\n",
    "- **Advanced (requires more resources)**: `\"microsoft/DialoGPT-large\"`, `\"facebook/blenderbot-1B-distill\"`\n",
    "\n",
    "### Embedding Models:\n",
    "- **Fast**: `\"sentence-transformers/all-MiniLM-L6-v2\"`\n",
    "- **Better Quality**: `\"sentence-transformers/all-mpnet-base-v2\"`\n",
    "- **Specialized**: `\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"` (for Q&A)\n",
    "\n",
    "### Using Hugging Face Account (Optional):\n",
    "If you have a Hugging Face account and token, you can access more models:\n",
    "```python\n",
    "# Get your token from https://huggingface.co/settings/tokens\n",
    "rag_assistant = HuggingFaceRAGAssistant(\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",  # Requires HF token\n",
    "    use_auth_token=\"your_hf_token_here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c90e95",
   "metadata": {},
   "source": [
    "## Load and Process Documents\n",
    "\n",
    "> Now let's load a PDF document and create a vector store for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7d8eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found PDF file: ../data/CUI_SPEC.pdf\n",
      "🔄 Processing PDF: ../data/CUI_SPEC.pdf\n",
      "📄 Loaded 29 pages from PDF\n",
      "✂️  Split into 86 chunks\n",
      "✅ Vector store created successfully!\n",
      "\n",
      "📊 Processing Results:\n",
      "   📄 Total documents/chunks: 86\n",
      "   🔍 Vector store created: ✅ Yes\n"
     ]
    }
   ],
   "source": [
    "# Configure document processing\n",
    "pdf_file_path = \"../data/CUI_SPEC.pdf\"  # Adjust path as needed\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(pdf_file_path):\n",
    "    print(f\"📁 Found PDF file: {pdf_file_path}\")\n",
    "    \n",
    "    # Process the PDF and create vector store\n",
    "    documents, vector_store = rag_assistant.process_pdf_to_vector_store(\n",
    "        pdf_path=pdf_file_path,\n",
    "        chunk_size=1000,  # Adjust based on your needs\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Processing Results:\")\n",
    "    print(f\"   📄 Total documents/chunks: {len(documents)}\")\n",
    "    print(f\"   🔍 Vector store created: {'✅ Yes' if vector_store else '❌ No'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ PDF file not found: {pdf_file_path}\")\n",
    "    print(\"📝 Please ensure the file exists or update the path.\")\n",
    "    print(\"🔄 You can also upload your own PDF file to the data folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b335c5c",
   "metadata": {},
   "source": [
    "## Examine the Processed Documents\n",
    "\n",
    "> Let's take a look at what documents were created from the PDF processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b844f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Examining processed documents:\n",
      "   Total chunks: 86\n",
      "\n",
      "📄 Document Chunk 1:\n",
      "   Content length: 956 characters\n",
      "   Preview: AVAILABLE ONLINE AT: INITIATED BY: \n",
      "www.directives.doe.gov Office of the Chief Information Officer \n",
      "U.S. Department of Energy ORDE R \n",
      "Washington, DC \n",
      " Approved: 2-3-2022 \n",
      "SUBJECT: CONTROLLED UNCLASSIF...\n",
      "   Metadata: {'source': '../data/CUI_SPEC.pdf', 'page': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 Document Chunk 2:\n",
      "   Content length: 941 characters\n",
      "   Preview: the Atomic Energy Act of 1954 (42 U.S.C. 2011, et seq.), as amended. This Directive\n",
      "implements the requirements in EO 13556, Controlled Unclassified Information, and 32\n",
      "CFR part 2002, Controlled Uncla...\n",
      "   Metadata: {'source': '../data/CUI_SPEC.pdf', 'page': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📄 Document Chunk 3:\n",
      "   Content length: 952 characters\n",
      "   Preview: commitment is modified to either eliminate requirements that are no longer applicable\n",
      "or substitute a new set of requirements.\n",
      "3. APPLICABILITY.\n",
      "a. Departmental  Applicability. This directive applies ...\n",
      "   Metadata: {'source': '../data/CUI_SPEC.pdf', 'page': 0}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if documents:\n",
    "    print(f\"📚 Examining processed documents:\")\n",
    "    print(f\"   Total chunks: {len(documents)}\")\n",
    "    \n",
    "    # Show first few document chunks\n",
    "    num_to_show = min(3, len(documents))\n",
    "    \n",
    "    for i, doc in enumerate(documents[:num_to_show]):\n",
    "        print(f\"\\n📄 Document Chunk {i + 1}:\")\n",
    "        print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "        print(f\"   Preview: {doc.page_content[:200]}...\")\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "            print(f\"   Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"❌ No documents to examine. Please process a PDF first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733a21f",
   "metadata": {},
   "source": [
    "## Test Vector Store Queries\n",
    "\n",
    "> Let's test how well our vector store can find relevant documents for specific queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a8dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing vector store queries:\n",
      "============================================================\n",
      "\n",
      "🔍 Query 1: What is CUI Specified?\n",
      "----------------------------------------\n",
      "🔍 Found 2 relevant documents\n",
      "   Document 1: Similarity score = 0.6325\n",
      "   Document 2: Similarity score = 0.6962\n",
      "📄 Result 1 (Score: 0.6325):\n",
      "   Attachment 2  Page 2-2  DOE O 471.7  2-3-2022  9. CUI Specified. This is the subset of CUI in which the authorizing law, regulation, or Government-wide policy contains specific handling controls that it requires or permits agencies to use that differ from those for CUI Basic. 10. CUI Marking Handboo...\n",
      "\n",
      "📄 Result 2 (Score: 0.6962):\n",
      "   3 DOE O 471.7  2-3-2022  requirements. However, authorized holders may only handle CUI when  furthering a LGP. At DOE, an authorized holder’s LGP may be defined by  DOE policy, position descriptions, or contractual requirements.   (4) For information to be identified as CUI, it must be designated as...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Query 2: Tell me about sending CUI via email to accounts outside of Federal IT\n",
      "----------------------------------------\n",
      "🔍 Found 2 relevant documents\n",
      "   Document 1: Similarity score = 0.6947\n",
      "   Document 2: Similarity score = 0.7839\n",
      "📄 Result 1 (Score: 0.6947):\n",
      "   5 DOE O 471.7  2-3-2022  (FOIA), and Mandatory Declassification Reviews (MDR) procedures, of  CUI must be approved by the Departmental Element Designated CUI  Official.  (4) When transmitting CUI by mail, it can be shipped through interagency mail systems, United States Postal Service (USPS), or oth...\n",
      "\n",
      "📄 Result 2 (Score: 0.7839):\n",
      "   Attachment 1 – CRD, Contractors Only DOE O 471.7  2-3-2022 Page 1-3  have a LGP. Additionally, access to CUI is restricted in accordance with  applicable LRGWP.  h. Dissemination. [Limits on how CUI is transmitted.] (1) When sending CUI via email to accounts outside of Federal IT systems the CUI mus...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Query 3: What are the handling requirements for CUI?\n",
      "----------------------------------------\n",
      "🔍 Found 2 relevant documents\n",
      "   Document 1: Similarity score = 0.7275\n",
      "   Document 2: Similarity score = 0.7341\n",
      "📄 Result 1 (Score: 0.7275):\n",
      "   Attachment 2  Page 2-2  DOE O 471.7  2-3-2022  9. CUI Specified. This is the subset of CUI in which the authorizing law, regulation, or Government-wide policy contains specific handling controls that it requires or permits agencies to use that differ from those for CUI Basic. 10. CUI Marking Handboo...\n",
      "\n",
      "📄 Result 2 (Score: 0.7341):\n",
      "   as or is not identified as CUI, as appropriate. (3) Report the misuse of CUI under their Departmental Element’s procedures, in printed or electronic form. (4) Complete CUI training required under this Directive and applicable LRGWPs. (5) Comply with 32 CFR part 2002 and applicable LRGWPs. (6) Coordi...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Query 4: What is controlled unclassified information?\n",
      "----------------------------------------\n",
      "🔍 Found 2 relevant documents\n",
      "   Document 1: Similarity score = 0.7791\n",
      "   Document 2: Similarity score = 0.8514\n",
      "📄 Result 1 (Score: 0.7791):\n",
      "   designate or handle CUI, consistent with the guidelines in this Directive, 32 CFR Part 2002, and the CUI Registry. 3. Control Level. A general term that indicates the safeguarding and disseminating requirements associated with CUI Basic and CUI Specified. 4. Controlled Environment. Any area or space...\n",
      "\n",
      "📄 Result 2 (Score: 0.8514):\n",
      "   Attachment 2 – Federal Employees and Contractors DOE O 471.7  2-3-2022 Page 2-3 (and 2-4)  19. Law, Regulation, or Government- wide Policy (LRGWP). The basis for safeguarding or dissemination controls under 32 CFR part 2002.  20. Legacy Material. Unclassified information (e.g., OUO) that is marked a...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if rag_assistant.vector_store is not None:\n",
    "    # Test queries related to the CUI_SPEC.pdf document\n",
    "    test_queries = [\n",
    "        \"What is CUI Specified?\",\n",
    "        \"Tell me about sending CUI via email to accounts outside of Federal IT\",\n",
    "        \"What are the handling requirements for CUI?\",\n",
    "        \"What is controlled unclassified information?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 Testing vector store queries:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n🔍 Query {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Search for relevant documents\n",
    "        docs, scores = rag_assistant.query_vector_store(\n",
    "            query=query,\n",
    "            k=2,  # Get top 2 most relevant documents\n",
    "            min_score=0.0\n",
    "        )\n",
    "        \n",
    "        if docs:\n",
    "            for j, (doc, score) in enumerate(zip(docs, scores)):\n",
    "                print(f\"📄 Result {j + 1} (Score: {score:.4f}):\")\n",
    "                # Show first 300 characters of the document\n",
    "                preview = doc.page_content[:300].replace('\\n', ' ')\n",
    "                print(f\"   {preview}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"   ❌ No relevant documents found\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"❌ Vector store not available. Please process a document first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fd428",
   "metadata": {},
   "source": [
    "## Create Interactive Gradio Interface\n",
    "\n",
    "> Now let's create a user-friendly Gradio interface to interact with our RAG assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eaf9dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Creating Gradio interface...\n",
      "✅ Interface created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_rag_interface():\n",
    "    \"\"\"Create a Gradio interface for the RAG assistant.\"\"\"\n",
    "    \n",
    "    def chat_with_assistant(message, use_rag, temperature, top_k, top_p, max_tokens, k_docs):\n",
    "        \"\"\"Handle chat interactions.\"\"\"\n",
    "        # Update assistant parameters\n",
    "        rag_assistant.temperature = temperature\n",
    "        rag_assistant.top_k = int(top_k)\n",
    "        rag_assistant.top_p = top_p\n",
    "        rag_assistant.max_new_tokens = int(max_tokens)\n",
    "        rag_assistant.k = int(k_docs)\n",
    "        \n",
    "        # Generate response\n",
    "        response = rag_assistant.generate_response(message, use_rag=use_rag)\n",
    "        \n",
    "        # Add mode indicator\n",
    "        mode = \"🔍 RAG Mode\" if use_rag else \"🤖 Standard Mode\"\n",
    "        return f\"{mode}\\n\\n{response}\"\n",
    "    \n",
    "    def process_new_pdf(pdf_file, chunk_size, chunk_overlap):\n",
    "        \"\"\"Process a new PDF file.\"\"\"\n",
    "        if pdf_file is None:\n",
    "            return \"❌ Please upload a PDF file.\"\n",
    "        \n",
    "        try:\n",
    "            # Save uploaded file temporarily\n",
    "            temp_path = f\"temp_{pdf_file.name}\"\n",
    "            with open(temp_path, \"wb\") as f:\n",
    "                f.write(pdf_file.read())\n",
    "            \n",
    "            # Process the PDF\n",
    "            documents, vector_store = rag_assistant.process_pdf_to_vector_store(\n",
    "                pdf_path=temp_path,\n",
    "                chunk_size=int(chunk_size),\n",
    "                chunk_overlap=int(chunk_overlap)\n",
    "            )\n",
    "            \n",
    "            # Clean up temp file\n",
    "            os.remove(temp_path)\n",
    "            \n",
    "            if documents:\n",
    "                return f\"✅ Successfully processed PDF!\\n📄 Created {len(documents)} document chunks.\\n🔍 Vector store ready for RAG queries.\"\n",
    "            else:\n",
    "                return \"❌ Failed to process PDF. Please try again.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"❌ Error processing PDF: {str(e)}\"\n",
    "    \n",
    "    # Create the Gradio interface\n",
    "    with gr.Blocks(title=\"RAG Assistant - Hugging Face Models\", theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\"# 🤖 RAG Assistant with Hugging Face Models\")\n",
    "        gr.Markdown(\"This interface allows you to chat with an AI assistant that can use Retrieval Augmented Generation (RAG) to answer questions based on your documents.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            # Left column - Controls\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"## ⚙️ Settings\")\n",
    "                \n",
    "                # RAG toggle\n",
    "                rag_enabled = gr.Checkbox(\n",
    "                    label=\"🔍 Enable RAG Mode\",\n",
    "                    value=False,\n",
    "                    info=\"Use document knowledge for responses\"\n",
    "                )\n",
    "                \n",
    "                # Generation parameters\n",
    "                gr.Markdown(\"### 🎛️ Generation Parameters\")\n",
    "                temperature = gr.Slider(0.1, 2.0, value=0.7, label=\"Temperature\", info=\"Creativity level\")\n",
    "                top_k = gr.Slider(1, 100, value=50, label=\"Top K\", info=\"Token selection diversity\")\n",
    "                top_p = gr.Slider(0.1, 1.0, value=0.9, label=\"Top P\", info=\"Nucleus sampling\")\n",
    "                max_tokens = gr.Slider(50, 1000, value=512, label=\"Max Tokens\", info=\"Response length\")\n",
    "                \n",
    "                # RAG parameters\n",
    "                gr.Markdown(\"### 🔍 RAG Parameters\")\n",
    "                k_docs = gr.Slider(1, 10, value=3, label=\"K Documents\", info=\"Number of docs to retrieve\")\n",
    "                \n",
    "                # Document upload\n",
    "                gr.Markdown(\"### 📄 Document Management\")\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                chunk_size = gr.Number(value=1000, label=\"Chunk Size\")\n",
    "                chunk_overlap = gr.Number(value=200, label=\"Chunk Overlap\")\n",
    "                process_btn = gr.Button(\"📤 Process PDF\")\n",
    "                process_status = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
    "            \n",
    "            # Right column - Chat\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"## 💬 Chat Interface\")\n",
    "                \n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(height=400, label=\"Conversation\", type='messages')\n",
    "                user_input = gr.Textbox(\n",
    "                    label=\"Your Message\",\n",
    "                    placeholder=\"Ask a question or chat with the assistant...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    send_btn = gr.Button(\"📤 Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"🗑️ Clear Chat\")\n",
    "                \n",
    "                # Sample questions\n",
    "                gr.Markdown(\"### 💡 Sample Questions (for CUI_SPEC.pdf)\")\n",
    "                sample_questions = [\n",
    "                    \"What is CUI?\",\n",
    "                    \"How should CUI be handled when sending emails?\",\n",
    "                    \"What are the marking requirements for CUI?\",\n",
    "                    \"Explain the safeguarding requirements for CUI.\"\n",
    "                ]\n",
    "                \n",
    "                for question in sample_questions:\n",
    "                    gr.Button(question, size=\"sm\").click(\n",
    "                        lambda q=question: q, outputs=user_input\n",
    "                    )\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, history, use_rag, temp, top_k_val, top_p_val, max_tok, k_val):\n",
    "            if not message:\n",
    "                return history, \"\"\n",
    "            \n",
    "            # Get response from assistant\n",
    "            response = chat_with_assistant(message, use_rag, temp, top_k_val, top_p_val, max_tok, k_val)\n",
    "            \n",
    "            # Update chat history\n",
    "            history.append({\"role\": \"user\", \"content\": message})\n",
    "            history.append({\"role\": \"assistant\", \"content\": response})\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Connect events\n",
    "        send_btn.click(\n",
    "            respond,\n",
    "            inputs=[user_input, chatbot, rag_enabled, temperature, top_k, top_p, max_tokens, k_docs],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            respond,\n",
    "            inputs=[user_input, chatbot, rag_enabled, temperature, top_k, top_p, max_tokens, k_docs],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, user_input])\n",
    "        \n",
    "        process_btn.click(\n",
    "            process_new_pdf,\n",
    "            inputs=[pdf_upload, chunk_size, chunk_overlap],\n",
    "            outputs=process_status\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Create and display the interface\n",
    "print(\"🎨 Creating Gradio interface...\")\n",
    "rag_interface = create_rag_interface()\n",
    "print(\"✅ Interface created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6f98a",
   "metadata": {},
   "source": [
    "## Launch the Interactive Application\n",
    "\n",
    "> **Note**: The app will launch on a unique port. You can use it to:\n",
    "> 1. Toggle between RAG mode and standard mode\n",
    "> 2. Adjust generation parameters (temperature, top-k, top-p)\n",
    "> 3. Upload and process new PDF documents\n",
    "> 4. Ask questions and compare responses with and without RAG\n",
    "\n",
    "### How to use the interface:\n",
    "\n",
    "1. **Standard Mode**: Ask general questions using the model's built-in knowledge\n",
    "2. **RAG Mode**: Enable RAG to use document knowledge for responses\n",
    "3. **Upload PDFs**: Use the upload section to process your own documents\n",
    "4. **Adjust Parameters**: Fine-tune the model's behavior using the sliders\n",
    "\n",
    "### Tips for best results:\n",
    "- Start with RAG disabled to see baseline responses\n",
    "- Enable RAG and ask the same questions to see the difference\n",
    "- Try the sample questions provided for the CUI document\n",
    "- Experiment with different parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d081d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Launching RAG Assistant interface...\n",
      "📱 The interface will open in a new tab/window\n",
      "🔗 You can also access it through the provided local URL\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3c4fff20c84badceda.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3c4fff20c84badceda.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Found 3 relevant documents\n",
      "   Document 1: Similarity score = 0.8217\n",
      "   Document 2: Similarity score = 0.9151\n",
      "   Document 3: Similarity score = 0.9331\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "if rag_interface is not None:\n",
    "    print(\"🚀 Launching RAG Assistant interface...\")\n",
    "    print(\"📱 The interface will open in a new tab/window\")\n",
    "    print(\"🔗 You can also access it through the provided local URL\")\n",
    "    \n",
    "    # Launch with sharing enabled for broader access\n",
    "    rag_interface.launch(\n",
    "        share=True,  # Creates a public link for 72 hours\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        debug=False,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ Failed to create interface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78c0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "🛑 Interface stopped\n"
     ]
    }
   ],
   "source": [
    "# If you need to stop the interface, run this cell\n",
    "if 'rag_interface' in locals():\n",
    "    rag_interface.close()\n",
    "    print(\"🛑 Interface stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bbfe4d",
   "metadata": {},
   "source": [
    "## Test RAG vs Non-RAG Responses\n",
    "\n",
    "> Let's directly compare responses with and without RAG to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_assistant.vector_store is not None:\n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"What is CUI?\",\n",
    "        \"How should I handle CUI when sending emails outside the organization?\",\n",
    "        \"What are the marking requirements for controlled unclassified information?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🔬 Comparing RAG vs Non-RAG responses:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n❓ Question {i}: {question}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get response without RAG\n",
    "        print(\"🤖 Standard Response (No RAG):\")\n",
    "        standard_response = rag_assistant.generate_response(question, use_rag=False)\n",
    "        print(f\"   {standard_response}\")\n",
    "        \n",
    "        print(\"\\n🔍 RAG-Enhanced Response:\")\n",
    "        rag_response = rag_assistant.generate_response(question, use_rag=True)\n",
    "        print(f\"   {rag_response}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"❌ Vector store not available. Please process a document first to compare RAG vs non-RAG responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43cc91",
   "metadata": {},
   "source": [
    "## System Information and Troubleshooting\n",
    "\n",
    "> Check system resources and get troubleshooting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get system information for troubleshooting.\"\"\"\n",
    "    print(\"🖥️  System Information:\")\n",
    "    print(f\"   Platform: {platform.platform()}\")\n",
    "    print(f\"   Python version: {platform.python_version()}\")\n",
    "    print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
    "    print(f\"   RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    print(f\"   Available RAM: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    else:\n",
    "        print(\"   GPU: Not available (using CPU)\")\n",
    "    \n",
    "    print(f\"\\n📦 Key Package Versions:\")\n",
    "    print(f\"   torch: {torch.__version__}\")\n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"   transformers: {transformers.__version__}\")\n",
    "    except:\n",
    "        print(\"   transformers: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import sentence_transformers\n",
    "        print(f\"   sentence-transformers: {sentence_transformers.__version__}\")\n",
    "    except:\n",
    "        print(\"   sentence-transformers: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import langchain\n",
    "        print(f\"   langchain: {langchain.__version__}\")\n",
    "    except:\n",
    "        print(\"   langchain: Not installed\")\n",
    "\n",
    "# Display system information\n",
    "get_system_info()\n",
    "\n",
    "# Show current model status\n",
    "print(f\"\\n🤖 Current Model Status:\")\n",
    "print(f\"   Language Model: {rag_assistant.model_name}\")\n",
    "print(f\"   Embedding Model: {rag_assistant.embedding_model_name}\")\n",
    "print(f\"   Device: {rag_assistant.device}\")\n",
    "print(f\"   Vector Store: {'✅ Loaded' if rag_assistant.vector_store else '❌ Not loaded'}\")\n",
    "print(f\"   Documents: {len(rag_assistant.documents)} chunks\")\n",
    "print(f\"   RAG Mode: {'✅ Enabled' if rag_assistant.rag_mode else '❌ Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28e48a",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "1. **Out of Memory Errors**:\n",
    "   - Use smaller models (e.g., \"gpt2\" instead of larger models)\n",
    "   - Reduce `max_new_tokens` parameter\n",
    "   - Use CPU instead of GPU if GPU memory is limited\n",
    "\n",
    "2. **Slow Response Times**:\n",
    "   - Use smaller, faster models\n",
    "   - Reduce chunk sizes when processing documents\n",
    "   - Lower the number of retrieved documents (k parameter)\n",
    "\n",
    "3. **Model Not Found Errors**:\n",
    "   - Check your internet connection\n",
    "   - Verify model names are correct\n",
    "   - Some models may require Hugging Face authentication\n",
    "\n",
    "4. **Poor RAG Performance**:\n",
    "   - Try different embedding models\n",
    "   - Adjust chunk sizes (smaller for specific questions, larger for context)\n",
    "   - Experiment with different similarity thresholds\n",
    "\n",
    "5. **Authentication Issues**:\n",
    "   - Get a free Hugging Face account and token\n",
    "   - Set the `use_auth_token` parameter when initializing the assistant\n",
    "\n",
    "### Performance Tips:\n",
    "- Start with lightweight models and upgrade as needed\n",
    "- Use GPU acceleration when available\n",
    "- Process documents in smaller batches for large files\n",
    "- Cache models locally for faster subsequent loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bda15ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Resources cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Clean up resources (optional)\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up memory and resources.\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    # Clear CUDA cache if using GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"🧹 Resources cleaned up\")\n",
    "\n",
    "# Uncomment the line below if you want to clean up resources\n",
    "cleanup_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56856bc-a973-4cc0-8e98-65c9bdedfec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ragworkshopvenv)",
   "language": "python",
   "name": "ragworkshopvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
