{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43217cfb-508b-43db-9393-72203c24222c",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Playground\n",
    "> This notebook acts as a tool for users to play around with vectorizing documents and using a RAG architecture to improve the responses and capabilities of an AI (LLM) for some unique purpose. The goal is to allow users to run the commands to load and vectorize their own documents and test how things such as chunking and retrieval parameters can improve the responses of LLM for unique contexts and subject matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4931fb-3127-4b6c-84eb-a7f8bbb1cbd3",
   "metadata": {},
   "source": [
    "## Load your model and set it's generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bd1a26-e6f7-48c7-a2b1-81b6eeea1ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 16:51:04.168109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-23 16:51:04.741192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t--->Try count: 0\n",
      "GRADIO_CACHE for gr.Image is: /home/gerald/gradio_cache\n",
      "GRADIO_CACHE for gr.Image is: /home/gerald/gradio_cache\n",
      "thing: 0\n",
      "gpu int: 0\n",
      "allocated_mem: 512\n",
      "reserved_mem: 2097152\n",
      "total_mem: 47697362944\n",
      "free_mem: 47695265280\n",
      "thing: 1\n",
      "gpu int: 1\n",
      "allocated_mem: 0\n",
      "reserved_mem: 0\n",
      "total_mem: 47697362944\n",
      "free_mem: 47697362944\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tAssigning model to GPU 1 with 44.42 GB free memory.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attempting to load remote version of '/home/gerald/shared_space/models/MODELS/meta_llama_Llama_3p2_1B_Instruct/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Callable Parameters: odict_keys(['text_inputs', 'kwargs'])\n",
      "Pipeline Model Configuration: LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"LlamaModel\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "✅ Successfully loaded remote version of '/home/gerald/shared_space/models/MODELS/meta_llama_Llama_3p2_1B_Instruct/'\n",
      "Max input tokens set to: 131072\n",
      "Max input tokens set to: 131072\n",
      "my assistant: <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4b7a143bb0>\n"
     ]
    }
   ],
   "source": [
    "from amas_manager_tools import *\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "import joblib\n",
    "from gradio_assistant_apps import *\n",
    "\n",
    "# Load an LLM model and set the generation parameters\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_path = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# model_path = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
    "# model_path = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# model_path = \"nvidia/Nemotron-Mini-4B-Instruct\"\n",
    "# model_path = \"nvidia/Mistral-NeMo-Minitron-8B-Instruct\"\n",
    "# model_path = \"mistralai/Mistral-7B-Instruct-v0.3\" \n",
    "model_path = \"/home/gerald/shared_space/models/MODELS/meta_llama_Llama_3p2_1B_Instruct/\"\n",
    "\n",
    "creds_json=\"../../data/credentials/HF_Tokens.json\"\n",
    "amas_assistant = AMAS_RAG_Assistant(\n",
    "    model_path=model_path,\n",
    "    # hf_login=True, creds_json=creds_json,\n",
    "    hf_login=False, creds_json=None,\n",
    "    max_tokens=4000, max_new_tokens=4000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c047d87-a20b-4518-9621-0c59a72e49e6",
   "metadata": {},
   "source": [
    "## Load a document, vectorize and store it for later retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad85a38-e711-4f50-bf9e-77c76e1164c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding model: /home/gerald/shared_space/models/MODELS/EMBEDING_MODELS/thenlper_gte_base/\n",
      "✅ Using previously saved wrapped model at: /home/gerald/shared_space/models/MODELS/EMBEDING_MODELS/thenlper_gte_base/\n"
     ]
    }
   ],
   "source": [
    "# now try to use the knexus manager to create and load a knexus\n",
    "\n",
    "pdf_file = [\n",
    "    # \"../../data/knowledge_docs/FIST_3-16_06-2020_Maintenace of Power circuit breakers BOR.pdf\",\n",
    "    \"../../data/knowledge_docs/CUI_SPEC.pdf\"\n",
    "]\n",
    "embedding_model_name = \"/home/gerald/shared_space/models/MODELS/EMBEDING_MODELS/thenlper_gte_large/\"\n",
    "# /home/gerald/shared_space/models/MODELS/\n",
    "embedding_model_name = \"/home/gerald/shared_space/models/MODELS/EMBEDING_MODELS/thenlper_gte_base/\"\n",
    "# embedding_model_name = \"../models/MODELS/EMBEDING_MODELS/sentence_transformers_multi_qa_mpnet_base_dot_v1/\" # decent\n",
    "# embedding_model_name = \"../models/MODELS/EMBEDING_MODELS/sentence_transformers_all_MiniLM_L6_v2/\" # decent\n",
    "documents, vector_store, embeddings_tool = amas_assistant.knexus_mngr.process_pdfs_to_vector_store_and_embeddings(\n",
    "    pdf_paths=pdf_file, \n",
    "    embedding_model_name=embedding_model_name, \n",
    "    chunk_size=50000, chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab553e-0685-4171-bba5-e6b0ad115b2e",
   "metadata": {},
   "source": [
    "## Start a gradio UI & test how the RAG architecture improves responses  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478bb669-5589-4176-9492-6eade79b66cd",
   "metadata": {},
   "source": [
    "## Add your assistant bot to a Gradio UI tools and create the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1c9ba-ac04-47ea-b5f3-ca98fb35a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio_assistant_app = RAGWorkshopUI(assistant_bot=amas_assistant, \n",
    "                                     embedding_model_name=embedding_model_name)\n",
    "\n",
    "gradio_assistant_app.create_app_tabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0668b39-8484-4501-b432-4a065aa42fa6",
   "metadata": {},
   "source": [
    "## Start you app and start testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185ba25-842c-4ec8-90d3-86ea34affa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradio_assistant_app.launch_app(share=True, debug=False, server_port=7869, server_name=None, system_directive=None, \n",
    "                   save_context=False, use_system_role=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gjlMLACT",
   "language": "python",
   "name": "gjlmlact"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
